{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becc9e33dec9e488",
   "metadata": {},
   "source": [
    "# Switching from OpenAI API to Zhipu AI API at the Lowest Cost\n",
    "This tutorial aims to assist everyone in quickly and cost-effectively migrating from the OpenAI interface to the Zhipu AI API interface. You can complete this task by following the two steps below. Make sure that before you proceed, you have registered on the Zhipu AI Open Platform and obtained the Zhipu AI API KEY by clicking on the top right corner of the official website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67fb1d59f46c17",
   "metadata": {},
   "source": [
    "## 1. Change API Endpoint or Install ZhipuAI SDK\n",
    "\n",
    "This section demonstrates common invocation methods, including Function Call, OpenAI API, and more. You can accomplish this by changing the API endpoint or installing the ZhipuAI SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1035f5479ce3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\".QIxRXZ9Q0HJU2cTw\", base_url=\"https://open.bigmodel.cn/api/paas/v4\")\n",
    "\n",
    "# from zhipuai import ZhipuAI \n",
    "# client = ZhipuAI(api_key=\"your ZhipuAI api key\")  #如果您使用 智谱AI 的SDK，请使用这个代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071d38504dc5a96",
   "metadata": {},
   "source": [
    "# 2. Select the Appropriate Model\n",
    "\n",
    "The GLM-4 series consists of five models (including one visual multimodal model), namely GLM-4-0520, GLM-4-Air, GLM-4-Airx, GLM-4-Flash, and the visual understanding model GLM-4-V. For the corresponding prices, please refer to the [official website](https://open.bigmodel.cn/pricing). The four models have certain differences:\n",
    "- GLM-4-0520: Our current most advanced and intelligent model, with a significant improvement in instruction obedience of 18.6%, equipped with 128k context, released on 20240605.\n",
    "- GLM-4V: Supports various image understanding tasks such as visual question answering, image captioning, visual positioning, and complex target detection, with 128k context.\n",
    "- GLM-4-Airx: The high-performance version of GLM-4-Air, with the same effect but 2.6 times faster inference speed, about 80 tokens/s, with 8k context.\n",
    "- GLM-4-Air: The version with the best cost-performance ratio, with comprehensive performance close to GLM-4, equipped with 128k context, fast speed, and affordable price.\n",
    "- GLM-4-Flash: The version suitable for simple tasks, the fastest speed, and the most affordable price, with 128k context.\n",
    "\n",
    "In actual use, you need to set the client to the corresponding model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6966560a213a65",
   "metadata": {},
   "source": [
    "## 3. Actual Demonstration Code\n",
    "\n",
    "After completing the above two steps, the model migration is finished. Here, I have written the simplest code implementation for you, which includes tool invocation and conventional dialogue functions. You can run this code to directly use the GLM-4 series models to perform the original tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550b4c0dbdcfb22e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T09:46:10.156872Z",
     "start_time": "2024-06-25T09:46:06.478122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='202408161236009105f99c7c5e4f89', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='喵喵喵，我是一个人工智能助手，全名为智谱清言，基于智谱 AI 公司于 2023 联合训练的语言模型 GLM-4 开发而成，可以针对用户的问题和要求提供适当的答复和支持。', role='assistant', function_call=None, tool_calls=None))], created=1723782962, model='glm-4-0520', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=51, prompt_tokens=26, total_tokens=77), request_id='202408161236009105f99c7c5e4f89')\n",
      "ChatCompletion(id='202408161236021d6c46adfb29452a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=None))], created=1723782963, model='glm-4', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=198, total_tokens=199), request_id='202408161236021d6c46adfb29452a')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\".QIxRXZ9Q0HJU2cTw\", base_url=\"https://open.bigmodel.cn/api/paas/v4\")\n",
    "\n",
    "\n",
    "def function_chat(use_stream=False):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"What's the Celsius temperature in San Francisco?\"\n",
    "        },\n",
    "        # Give Observations\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": None,\n",
    "            \"function_call\": None,\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_1717912616815\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"get_current_weather\",\n",
    "                        \"arguments\": \"{\\\"location\\\": \\\"San Francisco, CA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n",
    "                    },\n",
    "                    \"type\": \"function\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "\n",
    "        ## Add Observation Result if you need\n",
    "        # {\n",
    "        #     \"tool_call_id\": \"call_1717912616815\",\n",
    "        #     \"role\": \"tool\",\n",
    "        #     \"name\": \"get_current_weather\",\n",
    "        #     \"content\": \"23°C\",\n",
    "        # }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"format\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"format\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=use_stream,\n",
    "        max_tokens=256,\n",
    "        temperature=0.9,\n",
    "        presence_penalty=1.2,\n",
    "        top_p=0.1,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    if response:\n",
    "        if use_stream:\n",
    "            for chunk in response:\n",
    "                print(chunk)\n",
    "        else:\n",
    "            print(response)\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "\n",
    "def simple_chat(use_stream=False):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"请在你输出的时候都带上“喵喵喵”三个字，放在开头。\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你是谁\"\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-0520\",\n",
    "        messages=messages,\n",
    "        stream=use_stream,\n",
    "        max_tokens=256,\n",
    "        temperature=0.4,\n",
    "        presence_penalty=1.2,\n",
    "        top_p=0.8,\n",
    "    )\n",
    "    if response:\n",
    "        if use_stream:\n",
    "            for chunk in response:\n",
    "                print(chunk)\n",
    "        else:\n",
    "            print(response)\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_chat(use_stream=False)\n",
    "    function_chat(use_stream=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6e52deedc13be",
   "metadata": {},
   "source": [
    "## 3. Using with Open Source Frameworks\n",
    "\n",
    "The ZhipuAI API is aligned with the OpenAI API standard, and the GLM-4 series models are well-suited to handle tasks that can be created with the OpenAI API. You can also refer to the [sample code](./glm_langchain.ipynb) we provide.\n",
    "\n",
    "If you are using a framework other than LangChain, you only need to modify the corresponding node `base_url=\"https://open.bigmodel.cn/api/paas/v4\"` and adjust the model accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61d41015738011",
   "metadata": {},
   "source": [
    "# 4. Prompt Migration\n",
    "\n",
    "The GLM-4 series models still have some gaps compared to the GPT-4 flagship model. In terms of prompts, the GLM-4 model's instruction-following ability can handle a large number of conventional tasks. You can complete the migration from GPT2 to GLM without modifying the prompts.\n",
    "\n",
    "For a more cautious approach, the following suggestions can help the model perform better:\n",
    "- Add Few-Shot examples to the prompts; more examples can help the model understand the intent.\n",
    "- Emphasize the core content multiple times in the prompts to ensure better understanding by the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
